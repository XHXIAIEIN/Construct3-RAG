# RAG 项目原理讲解

## 什么是 RAG？

**RAG = Retrieval-Augmented Generation（检索增强生成）**

想象一下：你问一个 AI "Construct 3 里怎么让精灵跳起来？"

| 方式 | 问题 |
|------|------|
| 纯 LLM | AI 只能凭"记忆"回答，可能过时或瞎编 |
| RAG | AI 先去"翻书"找到相关资料，再基于资料回答 |

RAG 就像给 AI 配了一个**即时查阅资料库的能力**。

---

## 工作流程

```
用户提问 ──→ ① 检索 ──→ ② 增强 ──→ ③ 生成 ──→ 回答
              │          │          │
              ↓          ↓          ↓
           向量数据库   拼接上下文    LLM
```

### 第一阶段：数据准备（离线，只做一次）

```
原始文档              分块                   向量化                存储
(Markdown/CSV)  ──→  按 H2 标题切分  ──→  转成数字向量  ──→  Qdrant 数据库
                     小段落
```

**关键文件：**
- `indexer.py` - 负责整个索引流程
- `markdown_parser.py` - 把 Markdown 文档切成小块

---

## 核心概念详解

### 什么是向量化？

**向量化 = 把文字转换成一串数字**

#### 简单例子

```
"苹果"  →  [0.12, -0.45, 0.78, 0.33, ..., 0.21]   (1024个数字)
"水果"  →  [0.15, -0.42, 0.75, 0.30, ..., 0.19]   (1024个数字)
"汽车"  →  [0.89, 0.12, -0.56, 0.67, ..., 0.44]   (1024个数字)
```

#### 为什么要这样做？

**计算机不懂文字，但懂数字。**

| 问题 | 解决方案 |
|------|----------|
| 怎么比较两段文字是否相似？ | 比较它们的向量距离 |
| "苹果"和"水果"相关吗？ | 它们的向量很接近 ✓ |
| "苹果"和"汽车"相关吗？ | 它们的向量距离很远 ✗ |

#### 这些数字从哪来？

**这些数字是神经网络"学习"出来的，不是人为设定的。**

就像警察给嫌疑人画像：

```
身高: 1.75m
体重: 70kg
发型: 短发
...
```

Embedding 模型给文字"画像"：

```
"苹果" 的语义特征:
  - 是否是食物: 0.85 (是)
  - 是否是水果: 0.92 (是)
  - 是否是电子产品: 0.15 (不太是)
  - 是否是交通工具: -0.89 (完全不是)
  - 颜色倾向红/绿: 0.67
  - ...还有 1019 个维度
```

#### 模型是怎么学会的？

```
训练数据（海量文本）
    │
    │  "我吃了一个苹果"
    │  "苹果是一种水果"
    │  "橙子和苹果都是水果"
    │  "我开车去超市买苹果"
    │  ...数十亿句话
    │
    ▼
┌─────────────────────────┐
│   神经网络 (Transformer) │
│   反复调整内部参数        │
│   学习词语之间的关系      │
└─────────────────────────┘
    │
    ▼
学会了：
  - "苹果"和"水果"经常一起出现 → 向量接近
  - "苹果"和"汽车"很少一起出现 → 向量远离
```

#### 模型如何"无师自通"？

**核心原理：分布式假设（Distributional Hypothesis）**

> "一个词的含义由它出现的上下文决定"

模型通过观察词语在什么语境中出现，来学习它的语义——不是被"告诉"，而是从统计规律中自动发现。

**具体学习过程：**

```
训练数据中的句子                              模型学到的关联
─────────────────────────────────────────────────────────────

"苹果是红色的"                    ┐
"红彤彤的苹果"                    ├──→  苹果 ↔ 红色（向量接近）
"苹果熟了变红"                    ┘

"吃了一个苹果"                    ┐
"苹果很好吃"                      ├──→  苹果 ↔ 食物（向量接近）
"苹果可以榨汁"                    ┘

"苹果是一种水果"                  ┐
"水果店卖苹果橙子"                ├──→  苹果 ↔ 水果（向量接近）
"我爱吃各种水果尤其是苹果"        ┘

从未见过：
"苹果有四个轮子"                  ┐
"开苹果去上班"                    ├──→  苹果 ↔ 交通工具（向量远离）
"给苹果加油"                      ┘
```

**训练任务示例（预测填空）：**

```
任务：填空 "我吃了一个 [MASK]"

模型猜测：
  - 苹果: 0.15 ✓ (常见搭配，奖励)
  - 香蕉: 0.12 ✓ (合理，奖励)
  - 汽车: 0.0001 ✗ (不合理，惩罚)
  - 手机: 0.002 ✗ (不合理，惩罚)

通过数十亿次这样的猜测和纠错，
模型逐渐学会了什么词在什么语境下合理
```

**简单类比：**

就像一个外国人学中文：
- 从没人解释"苹果是什么"
- 但他读了几十亿句中文后
- 发现"苹果"总是和"吃、红、甜、水果"一起出现
- 从来不和"驾驶、充电、屏幕"一起出现
- 于是他**推断**出苹果是一种红色的可食用水果

模型就是这样"无师自通"的——不是被教导，而是从数据的统计规律中**涌现**出语义理解。

#### 具体数字的含义

**说实话：我们不知道每个数字具体代表什么。**

```
[0.12, -0.45, 0.78, 0.33, ..., 0.21]
  │      │     │
  │      │     └── 可能和"圆形"相关？
  │      └──────── 可能和"机械"负相关？
  └──────────────── 可能和"可食用"相关？

（这只是猜测，实际上是黑盒）
```

这就像大脑的神经元——我们知道它能识别苹果，但不知道每个神经元具体在做什么。

#### 重要的是整体模式

```python
苹果 = [0.12, -0.45, 0.78, ...]
橙子 = [0.15, -0.42, 0.75, ...]  # 很接近！
汽车 = [0.89,  0.12, -0.56, ...] # 完全不同

# 计算相似度
similarity(苹果, 橙子) = 0.95  # 非常相似
similarity(苹果, 汽车) = 0.12  # 很不相似
```

**一句话总结：** 这些数字 = 神经网络从海量文本中自动学习出的"语义指纹"，捕捉了词语在人类语言中的使用模式。

#### 形象比喻

把文字想象成**地图上的坐标点**：

```
        语义空间
          ↑
    水果 ● ● 苹果
          ● 橙子

                    ● 汽车
                    ● 卡车
    ──────────────────────→
```

- 意思相近的词，坐标点靠近
- 意思不同的词，坐标点远离

#### 在这个项目里

```python
# indexer.py 中的代码
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-m3")

text = "Sprite 是游戏中的精灵对象"
vector = model.encode(text)  # → [0.12, -0.45, ..., 0.21] (1024维)
```

**BGE-M3 模型**做的就是这件事：
- 输入：一段文字
- 输出：1024 个数字组成的向量
- 这些数字**编码了文字的语义含义**

#### 搜索时怎么用？

```
用户问: "怎么让角色跳跃？"
                ↓
        向量化: [0.33, 0.56, ...]
                ↓
        在数据库里找最接近的向量
                ↓
        找到: "Platform 行为可以让精灵跳跃..."
```

这就是**语义搜索**——不是匹配关键词，而是匹配**意思**。

### 什么是分块（Chunking）？

**分块 = 把长文档切成小段落**

#### 为什么要分块？

| 问题 | 原因 |
|------|------|
| LLM 上下文有限 | 不能把整本手册都塞进去 |
| 检索需要精确 | 整篇文章太大，找不到重点 |
| 向量表示有限 | 太长的文本，向量无法精确表达 |

#### 这个项目的分块策略：H2 标题切分

以实际的 `plugin-reference/sprite.md` 为例：

```markdown
# Sprite（精灵）                      ← H1 标题（文件级别）
Sprite 对象是项目中显示的可动画图像，用于制作玩家、敌人、子弹...

## Sprite properties（属性）          ← H2 → 第 1 块
**Initial animation** 设置初始显示的动画...

## Sprite conditions（条件）          ← H2 → 第 2 块
**On finished** 当动画播放结束时触发...

## Sprite actions（动作）             ← H2 → 第 3 块
**Set animation** 切换当前播放的动画...

## Sprite expressions（表达式）       ← H2 → 第 4 块
**AnimationName** 返回当前动画名称...
```

每个 H2 段落变成一个独立的"文档块"，存入向量数据库。

#### 分块的元数据

每个块不仅有内容，还保留了**上下文信息**：

```python
{
    "text": "## Sprite properties\\n**Initial animation** Set the initially...",  # 分块内容
    "metadata": {
        "title": "Sprite",                                # 文档标题
        "source": "plugin-reference/sprite.md",           # 来源文件路径（可推导 category/breadcrumb）
        "collection": "c3_plugins",                       # 向量集合名称
        "subcategory": "general",                         # 子分类（general/data/input/media...）
        "h1_heading": "Sprite",                           # H1 标题
        "h2_heading": "Sprite properties",                # H2 标题
        "section_type": "properties",                     # 分块类型（properties/conditions/actions/expressions）
    }
}
```

`category` 和 `breadcrumb` 不存储，使用时从 `source` 推导：
- `category`: `source.split('/')[0]` → `"plugin-reference"`
- `breadcrumb`: `source[:-3].split('/')` → `["plugin-reference", "sprite"]`

这样回答问题时，可以告诉用户"这个信息来自哪里"。

---

## 详细工作流程

### 第二阶段：检索（每次提问时）

```python
# retriever.py 的核心逻辑
def search_collection(query):
    query_vector = embedder.encode(query)       # 问题 → 向量
    results = qdrant.search(query_vector)       # 在数据库里找相似的
    return results                              # 返回最相关的 5 条
```

### 第三阶段：生成回答

```python
# chain.py 的核心逻辑
def answer(query):
    # 1. 检索相关文档
    context = retriever.search_all(query)

    # 2. 拼接提示词
    prompt = f"""
    参考资料：
    {context}

    用户问题：
    {query}

    请基于以上资料回答：
    """

    # 3. 让 LLM 生成回答
    answer = llm.generate(prompt)
    return answer
```

---

## 项目架构图

```
┌─────────────────────────────────────────────────────────┐
│                      Gradio Web 界面                      │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                    RAGChain (chain.py)                   │
│  ┌──────────────┐    ┌──────────────┐    ┌───────────┐ │
│  │ 问题分类      │ → │ 检索上下文    │ → │ LLM 生成   │ │
│  │ (qa/翻译/代码)│    │ (多集合搜索)  │    │ (Qwen3)   │ │
│  └──────────────┘    └──────────────┘    └───────────┘ │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│              HybridRetriever (retriever.py)              │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                  Qdrant 向量数据库                        │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐       │
│  │c3_guide │ │c3_plugins│ │c3_terms │ │c3_examples│      │
│  │入门教程  │ │插件参考  │ │术语翻译  │ │示例代码   │      │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘       │
└─────────────────────────────────────────────────────────┘
```

---

## 关键组件对应

| 组件 | 文件 | 作用 |
|------|------|------|
| 配置 | `config.py` | 模型路径、数据库地址 |
| 集合定义 | `collections.py` | 向量集合名称、目录映射、子分类 |
| 解析器 | `markdown_parser.py` | Markdown → 小块文本 |
| 索引器 | `indexer.py` | 文本 → 向量 → 存入 Qdrant |
| 检索器 | `retriever.py` | 问题 → 向量 → 搜索相似文档 |
| 生成链 | `chain.py` | 组合检索结果 + LLM 生成回答 |
| 界面 | `gradio_ui.py` | Web 交互界面 |

---

## 简单总结

```
RAG = 搜索引擎 + AI 写作

1. 预处理：把文档切块，转成向量存起来
2. 检索：用户提问时，找出最相关的文档片段
3. 生成：把找到的资料喂给 LLM，让它基于资料回答
```

这样 AI 的回答就能**有据可查**，还能标注来源，避免"幻觉"（瞎编）。

---

## 技术栈

| 组件 | 选择 | 说明 |
|------|------|------|
| LLM | qwen2.5:7b | 本地运行，通过 Ollama |
| Embedding | BAAI/bge-m3 | 多语言嵌入模型，1024 维 |
| 向量数据库 | Qdrant | 高性能向量搜索 |
| 文档解析 | Markdown | H2 语义分块 |
| 分块策略 | H2 语义分块 | 按文档结构切分 |
| 数据处理 | pandas, numpy | 数据清洗和处理 |
| 框架 | LangChain | RAG 编排框架 |
| 前端 | Gradio | 快速搭建 Web 界面 |
